#!/usr/bin/env python3
"""
C4 Parts Forecast: leakage-safe next-day prediction + Top-K service curves.

Inputs:
- CA_4_predict_daily_aggregate.csv
- CA_4_predict_mid_aggregate.csv
- CA_4_predict_eve_aggregate.csv

Targets:
- CA_QS1..CA_QS4 (categorical labels 0..9)

Aggregate features (per position):
- QS{pos}_{label} counts for labels 0..9

This script:
1) Validates dataset invariants (dates, ranges, row-sums with allowed exceptions).
2) Builds leakage-safe features at day t to predict day t+1.
3) Runs baselines + a pooled gradient-boosting model (position as a feature).
4) Produces Top-K curves and exclusion-risk curves per position.
5) Produces next-day recommendations: top-1, top-3, least-likely list.

Notes:
- Aggregates are assumed available at end of day t (before deciding shipment for t+1).
- Evaluation uses blocked time splits (no random shuffle).

Author: generated by ChatGPT
"""

from __future__ import annotations

import argparse
import json
import os
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import numpy as np
import pandas as pd
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import log_loss, top_k_accuracy_score


LABELS: Tuple[int, ...] = tuple(range(10))
N_CLASSES: int = 10
POS_COLS: Tuple[str, ...] = ("CA_QS1", "CA_QS2", "CA_QS3", "CA_QS4")


@dataclass(frozen=True)
class FileExpectations:
    """Expected aggregate row sums per position for each file type."""
    expected_sum: int
    tolerance: int


@dataclass(frozen=True)
class SplitSpec:
    """Blocked time split configuration."""
    test_days: int
    val_days: int


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    """
    Parse CLI arguments.

    Args:
        argv: Optional argv list.

    Returns:
        argparse.Namespace with config.
    """
    p = argparse.ArgumentParser(description="C4 parts forecast: top-k pipeline")
    p.add_argument("--daily_csv", type=str, default="CA_4_predict_daily_aggregate.csv")
    p.add_argument("--mid_csv", type=str, default="CA_4_predict_mid_aggregate.csv")
    p.add_argument("--eve_csv", type=str, default="CA_4_predict_eve_aggregate.csv")
    p.add_argument("--out_dir", type=str, default="artifacts")
    p.add_argument("--test_days", type=int, default=365)
    p.add_argument("--val_days", type=int, default=180)
    p.add_argument("--lags", type=int, nargs="+", default=[1, 2, 7, 14, 28])
    p.add_argument("--roll_windows", type=int, nargs="+", default=[7, 14, 30])
    p.add_argument("--topk", type=int, nargs="+", default=[1, 3, 5, 7, 9])
    p.add_argument("--least_m", type=int, default=7, help="Least-likely list size.")
    p.add_argument("--seed", type=int, default=42)
    return p.parse_args(argv)


def load_csv(path: str) -> pd.DataFrame:
    """
    Load a CSV and parse the date column.

    Args:
        path: CSV path.

    Returns:
        Sorted DataFrame with a datetime 'date' column.
    """
    df = pd.read_csv(path)
    if "date" not in df.columns:
        raise ValueError(f"Missing required column 'date' in {path}")
    df["date"] = pd.to_datetime(df["date"], errors="raise")
    return df.sort_values("date").reset_index(drop=True)


def ensure_daily_continuity(df: pd.DataFrame) -> None:
    """
    Ensure date is daily-continuous (no missing days).

    Args:
        df: Input DataFrame.

    Raises:
        ValueError if missing days exist.
    """
    dmin = df["date"].min()
    dmax = df["date"].max()
    expected = pd.date_range(dmin, dmax, freq="D")
    missing = expected.difference(df["date"])
    if len(missing) > 0:
        raise ValueError(f"Missing {len(missing)} dates, sample={missing[:5].to_list()}")


def validate_ranges(df: pd.DataFrame) -> None:
    """
    Validate CA target ranges and aggregate non-negativity.

    Args:
        df: Input DataFrame.

    Raises:
        ValueError on invalid ranges.
    """
    for c in POS_COLS:
        if c not in df.columns:
            raise ValueError(f"Missing target column {c}")
        bad = ~df[c].isin(LABELS)
        if bad.any():
            vals = df.loc[bad, c].unique().tolist()
            raise ValueError(f"Invalid values in {c}: {vals[:10]}")

    for pos in range(1, 5):
        for lab in LABELS:
            col = f"QS{pos}_{lab}"
            if col not in df.columns:
                raise ValueError(f"Missing aggregate column {col}")
            if (df[col] < 0).any():
                raise ValueError(f"Negative values found in {col}")


def validate_row_sums(
    df: pd.DataFrame,
    expectations: FileExpectations,
    allow_exceptions: bool = True,
) -> pd.DataFrame:
    """
    Validate aggregate row sums per position and return exceptions.

    Args:
        df: Input DataFrame.
        expectations: Expected sum and tolerance.
        allow_exceptions: If True, do not raise; return exception rows.

    Returns:
        DataFrame of exception rows with computed sums.
    """
    exc_rows: List[pd.DataFrame] = []
    for pos in range(1, 5):
        cols = [f"QS{pos}_{lab}" for lab in LABELS]
        sums = df[cols].sum(axis=1)
        df_tmp = df[["date"]].copy()
        df_tmp["position"] = pos
        df_tmp["row_sum"] = sums
        df_tmp["expected"] = expectations.expected_sum
        df_tmp["delta"] = df_tmp["row_sum"] - expectations.expected_sum
        mask = df_tmp["delta"].abs() > expectations.tolerance
        exc_rows.append(df_tmp.loc[mask])

    exc = pd.concat(exc_rows, axis=0).sort_values(["date", "position"]).reset_index(drop=True)

    if not allow_exceptions and len(exc) > 0:
        raise ValueError(
            f"Row-sum exceptions found (n={len(exc)}), "
            f"example={exc.head(5).to_dict(orient='records')}"
        )
    return exc


def make_supervised_features(
    df: pd.DataFrame,
    lags: Sequence[int],
    roll_windows: Sequence[int],
) -> pd.DataFrame:
    """
    Build a leakage-safe supervised dataset where features at day t predict targets at t+1.

    Features:
    - CA_QS{pos} lag features
    - Aggregate counts lag features
    - Aggregate proportions lag features
    - Rolling mean of aggregate proportions

    Args:
        df: Input daily DataFrame.
        lags: Lags to include.
        roll_windows: Rolling windows (in days).

    Returns:
        Supervised dataset with columns:
        - date
        - position (1..4)
        - y (target for t+1)
        - feature columns
    """
    df = df.copy()

    rows: List[pd.DataFrame] = []
    for pos in range(1, 5):
        ca_col = f"CA_QS{pos}"
        part_cols = [f"QS{pos}_{lab}" for lab in LABELS]

        tmp = pd.DataFrame({"date": df["date"].values})
        tmp["position"] = pos
        tmp["y"] = df[ca_col].shift(-1).astype("float")

        # CA lags
        for lag in lags:
            tmp[f"ca_lag{lag}"] = df[ca_col].shift(lag).astype("float")

        # Aggregate count lags & proportions
        denom = df[part_cols].sum(axis=1).replace(0, np.nan)
        for lab in LABELS:
            tmp[f"agg_count_{lab}_t"] = df[f"QS{pos}_{lab}"].astype("float")
            for lag in lags:
                tmp[f"agg_count_{lab}_lag{lag}"] = df[f"QS{pos}_{lab}"].shift(lag).astype("float")
                tmp[f"agg_prop_{lab}_lag{lag}"] = (df[f"QS{pos}_{lab}"].shift(lag) / denom.shift(lag)).astype("float")

            # Rolling mean of same-day proportions (computed from t and earlier)
            prop = (df[f"QS{pos}_{lab}"] / denom).astype("float")
            for w in roll_windows:
                tmp[f"agg_prop_{lab}_rollmean{w}"] = prop.shift(1).rolling(w).mean().astype("float")

        rows.append(tmp)

    out = pd.concat(rows, axis=0).sort_values(["date", "position"]).reset_index(drop=True)
    out = out.dropna(subset=["y"]).reset_index(drop=True)
    out["y"] = out["y"].astype(int)
    return out


def blocked_split(df: pd.DataFrame, spec: SplitSpec) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Create a blocked train/val/test split based on the end of the time series.

    Args:
        df: Supervised stacked dataset (date, position).
        spec: SplitSpec with val_days and test_days.

    Returns:
        (train_df, val_df, test_df)
    """
    max_date = df["date"].max()
    test_start = max_date - pd.Timedelta(days=spec.test_days) + pd.Timedelta(days=1)
    val_start = test_start - pd.Timedelta(days=spec.val_days)

    train_df = df[df["date"] < val_start].copy()
    val_df = df[(df["date"] >= val_start) & (df["date"] < test_start)].copy()
    test_df = df[df["date"] >= test_start].copy()

    if len(train_df) == 0 or len(val_df) == 0 or len(test_df) == 0:
        raise ValueError("Empty split; adjust --val_days/--test_days")

    return train_df, val_df, test_df


def softmax_rowwise(scores: np.ndarray) -> np.ndarray:
    """
    Softmax for a 2D matrix row-wise.

    Args:
        scores: (n, k) score matrix.

    Returns:
        (n, k) probabilities.
    """
    z = scores - np.max(scores, axis=1, keepdims=True)
    exp = np.exp(z)
    return exp / np.sum(exp, axis=1, keepdims=True)


def baseline_aggregate_ranker(df_daily: pd.DataFrame, day_t: pd.Series, pos: int) -> np.ndarray:
    """
    Baseline: use same-day aggregate counts as unnormalized scores for tomorrow.

    Args:
        df_daily: Full daily dataframe (for column access).
        day_t: Row corresponding to today (features available at end of day t).
        pos: Position index (1..4).

    Returns:
        Probability vector (10,).
    """
    scores = np.array([float(day_t[f"QS{pos}_{lab}"]) for lab in LABELS], dtype=float)
    if np.all(scores == 0):
        return np.ones(N_CLASSES, dtype=float) / float(N_CLASSES)
    return softmax_rowwise(scores.reshape(1, -1)).ravel()


def compute_topk_and_exclusion(
    y_true: np.ndarray,
    probs: np.ndarray,
    topk: Sequence[int],
) -> Tuple[Dict[str, float], Dict[str, float]]:
    """
    Compute top-k accuracy and exclusion risk curves.

    Exclusion risk for bottom-m:
    - risk_bottom_m = P(true in bottom-m least likely)

    Args:
        y_true: True labels (n,).
        probs: Probabilities (n, 10).
        topk: List of k values for top-k accuracy.

    Returns:
        (topk_metrics, exclusion_metrics)
    """
    topk_metrics: Dict[str, float] = {}
    for k in topk:
        k_eff = max(1, min(int(k), N_CLASSES))
        topk_metrics[f"top{k_eff}_acc"] = float(
            top_k_accuracy_score(y_true, probs, k=k_eff, labels=list(LABELS))
        )

    exclusion_metrics: Dict[str, float] = {}
    order = np.argsort(probs, axis=1)  # ascending: least likely first
    for m in range(1, N_CLASSES + 1):
        bottom = order[:, :m]
        hit = np.any(bottom == y_true.reshape(-1, 1), axis=1)
        exclusion_metrics[f"bottom{m}_risk"] = float(np.mean(hit))
    return topk_metrics, exclusion_metrics


def train_pooled_model(
    train_df: pd.DataFrame,
    val_df: pd.DataFrame,
    seed: int,
) -> HistGradientBoostingClassifier:
    """
    Train a pooled multiclass model using sklearn HistGradientBoostingClassifier.

    Args:
        train_df: Training set.
        val_df: Validation set (used for early stopping via scoring).
        seed: Random seed.

    Returns:
        Fitted classifier.
    """
    feature_cols = [c for c in train_df.columns if c not in ("date", "y")]
    X_train = train_df[feature_cols].to_numpy(dtype=float)
    y_train = train_df["y"].to_numpy(dtype=int)

    X_val = val_df[feature_cols].to_numpy(dtype=float)
    y_val = val_df["y"].to_numpy(dtype=int)

    clf = HistGradientBoostingClassifier(
        learning_rate=0.05,
        max_depth=6,
        max_iter=500,
        random_state=seed,
        early_stopping=True,
        validation_fraction=None,
        n_iter_no_change=25,
    )
    clf.fit(X_train, y_train)

    # sanity: val log loss
    val_probs = clf.predict_proba(X_val)
    _ = log_loss(y_val, val_probs, labels=list(LABELS))
    return clf


def save_json(path: str, obj: object) -> None:
    """
    Save object as pretty JSON.

    Args:
        path: Output path.
        obj: JSON-serializable object.
    """
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, default=str)


def main(argv: Optional[Sequence[str]] = None) -> int:
    """
    Run the pipeline.

    Args:
        argv: Optional argv list.

    Returns:
        Exit code.
    """
    args = parse_args(argv)
    os.makedirs(args.out_dir, exist_ok=True)
    np.random.seed(int(args.seed))

    daily = load_csv(args.daily_csv)
    mid = load_csv(args.mid_csv)
    eve = load_csv(args.eve_csv)

    # Basic validations
    for df in (daily, mid, eve):
        ensure_daily_continuity(df)
        validate_ranges(df)

    # Row-sum validations (exceptions are allowed; exported)
    exc_daily = validate_row_sums(daily, FileExpectations(expected_sum=37, tolerance=0))
    exc_mid = validate_row_sums(mid, FileExpectations(expected_sum=16, tolerance=0))
    exc_eve = validate_row_sums(eve, FileExpectations(expected_sum=21, tolerance=0))

    exc_all = pd.concat(
        [
            exc_daily.assign(file="daily"),
            exc_mid.assign(file="mid"),
            exc_eve.assign(file="eve"),
        ],
        axis=0,
    ).reset_index(drop=True)

    exc_csv = os.path.join(args.out_dir, "invariant_row_sum_exceptions.csv")
    exc_all.to_csv(exc_csv, index=False)

    # Supervised stacked dataset from DAILY (use daily as the main feature source)
    sup = make_supervised_features(daily, lags=args.lags, roll_windows=args.roll_windows)
    train_df, val_df, test_df = blocked_split(sup, SplitSpec(test_days=int(args.test_days), val_days=int(args.val_days)))

    feature_cols = [c for c in train_df.columns if c not in ("date", "y")]
    clf = train_pooled_model(train_df, val_df, seed=int(args.seed))

    # Evaluate per position on test
    results_topk: List[Dict[str, object]] = []
    results_excl: List[Dict[str, object]] = []

    for pos in range(1, 5):
        df_pos = test_df[test_df["position"] == pos].copy()
        X = df_pos[feature_cols].to_numpy(dtype=float)
        y = df_pos["y"].to_numpy(dtype=int)
        probs = clf.predict_proba(X)

        topk_m, excl_m = compute_topk_and_exclusion(y, probs, args.topk)
        results_topk.append({"position": pos, **topk_m})
        results_excl.append({"position": pos, **excl_m})

    topk_path = os.path.join(args.out_dir, "metrics_topk_by_position.csv")
    excl_path = os.path.join(args.out_dir, "metrics_exclusion_by_position.csv")

    pd.DataFrame(results_topk).to_csv(topk_path, index=False)
    pd.DataFrame(results_excl).to_csv(excl_path, index=False)

    # Next-day prediction using the last available day as "today"
    today_row = daily.sort_values("date").iloc[-1]
    today_date = pd.to_datetime(today_row["date"]).date()
    predict_date = (pd.to_datetime(today_row["date"]) + pd.Timedelta(days=1)).date()

    next_day_payload: List[Dict[str, object]] = []
    least_m = max(0, min(int(args.least_m), N_CLASSES))

    # Build a one-row-per-position feature frame corresponding to "today" features to predict tomorrow.
    # We'll re-use feature builder logic by taking a slice of daily and grabbing the last row from sup per position.
    sup_all = make_supervised_features(daily, lags=args.lags, roll_windows=args.roll_windows)
    last_by_pos = sup_all.sort_values("date").groupby("position").tail(1)

    for pos in range(1, 5):
        row = last_by_pos[last_by_pos["position"] == pos].iloc[0]
        X_row = row[feature_cols].to_numpy(dtype=float).reshape(1, -1)
        probs = clf.predict_proba(X_row).ravel()

        order_desc = np.argsort(-probs)
        order_asc = np.argsort(probs)

        top1 = int(order_desc[0])
        top3 = [int(x) for x in order_desc[:3]]
        least_likely = [int(x) for x in order_asc[:least_m]]

        next_day_payload.append(
            {
                "position": pos,
                "date_today": str(today_date),
                "date_predicting": str(predict_date),
                "top1_part": top1,
                "top3_parts": top3,
                "least_likely_parts": least_likely,
                "probs": {str(i): float(probs[i]) for i in LABELS},
            }
        )

    preds_path = os.path.join(args.out_dir, "next_day_predictions.json")
    save_json(preds_path, next_day_payload)

    # Also include an "aggregate-ranker" next-day suggestion for comparison
    agg_ranker_payload: List[Dict[str, object]] = []
    for pos in range(1, 5):
        probs = baseline_aggregate_ranker(daily, today_row, pos)
        order_desc = np.argsort(-probs)
        order_asc = np.argsort(probs)
        agg_ranker_payload.append(
            {
                "position": pos,
                "date_today": str(today_date),
                "date_predicting": str(predict_date),
                "top1_part": int(order_desc[0]),
                "top3_parts": [int(x) for x in order_desc[:3]],
                "least_likely_parts": [int(x) for x in order_asc[:least_m]],
                "probs": {str(i): float(probs[i]) for i in LABELS},
            }
        )

    agg_path = os.path.join(args.out_dir, "next_day_predictions_aggregate_ranker.json")
    save_json(agg_path, agg_ranker_payload)

    summary = {
        "artifacts": {
            "row_sum_exceptions_csv": exc_csv,
            "metrics_topk_by_position_csv": topk_path,
            "metrics_exclusion_by_position_csv": excl_path,
            "next_day_predictions_json": preds_path,
            "next_day_predictions_aggregate_ranker_json": agg_path,
        },
        "split": {
            "train_start": str(train_df["date"].min().date()),
            "train_end": str(train_df["date"].max().date()),
            "val_start": str(val_df["date"].min().date()),
            "val_end": str(val_df["date"].max().date()),
            "test_start": str(test_df["date"].min().date()),
            "test_end": str(test_df["date"].max().date()),
        },
    }
    save_json(os.path.join(args.out_dir, "run_summary.json"), summary)

    print(json.dumps(summary, indent=2))
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

# requirements.txt (copy/paste)
# numpy==1.26.4
# pandas==2.2.2
# scikit-learn==1.5.1
